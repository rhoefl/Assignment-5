import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

df = pd.read_csv("/content/Housing (2).csv")

features = ["area", "bedrooms", "bathrooms", "stories", "parking"]
target = "price"


df = df[features + [target]].dropna().copy()

X = df[features].values.astype("float32")
y = df[target].values.astype("float32").reshape(-1, 1)
X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(
    X, y, test_size=0.2, random_state=42
)

x_scaler = StandardScaler()
X_train_np = x_scaler.fit_transform(X_train_np).astype("float32")
X_val_np   = x_scaler.transform(X_val_np).astype("float32")
y_scaler = StandardScaler()
y_train_np = y_scaler.fit_transform(y_train_np).astype("float32")
y_val_np   = y_scaler.transform(y_val_np).astype("float32")
X_train = torch.tensor(X_train_np)
y_train = torch.tensor(y_train_np)
X_val   = torch.tensor(X_val_np)
y_val   = torch.tensor(y_val_np)

def model(X, w, b):
    return X @ w.view(-1,1) + b

def mse_loss(y_hat, y):
    return ((y_hat - y) ** 2).mean()

def r2_score_torch(y_hat, y):
    ss_res = ((y - y_hat) ** 2).sum()
    ss_tot = ((y - y.mean()) ** 2).sum()
    return 1.0 - ss_res / (ss_tot + 1e-12)
def train_once(lr=1e-4, epochs=5000, print_every=500):
    w = torch.zeros(5, requires_grad=True, dtype=torch.float32)
    b = torch.zeros((), requires_grad=True, dtype=torch.float32)

    history = []
    for epoch in range(epochs+1):
        y_hat = model(X_train, w, b)
        loss  = mse_loss(y_hat, y_train)
        loss.backward()

        with torch.no_grad():
            w -= lr * w.grad
            b -= lr * b.grad

        w.grad.zero_()
        b.grad.zero_()

        if epoch % print_every == 0:
            with torch.no_grad():
                y_val_hat = model(X_val, w, b)
                val_loss = mse_loss(y_val_hat, y_val)
                val_r2   = r2_score_torch(y_val_hat, y_val)
            print(f"lr={lr:.4g}, epoch={epoch:4d}, "
                  f"train_loss={loss.item():.4f}, val_loss={val_loss.item():.4f}, val_R2={val_r2.item():.4f}")
            history.append((epoch, float(loss.item()), float(val_loss.item()), float(val_r2.item())))
    return (w.detach(), float(b.detach().item())), history

lrs = [0.1, 0.01, 0.001, 0.0001]

results = {}
best_key, best_val = None, float("inf")

for lr in lrs:
    print(f"Training with learning rate = {lr}")
    params, history = train_once(lr=lr, epochs=5000, print_every=500)
    results[lr] = {"params": params, "history": history}

    final_val_loss = history[-1][2] 
    if final_val_loss < best_val:
        best_val = final_val_loss
        best_key = lr

print("\nBest model")
print(f"Best LR: {best_key}, final val_loss: {best_val:.4f}")
best_w, best_b = results[best_key]["params"]
print("Weights (W1..W5):", best_w.numpy()) 
print("Bias (B):", best_b)

from pprint import pprint
print("\nLoss/RÂ² summary (every 500 epochs):")
for lr in lrs:
    print(f"\nLR = {lr}")
    print("(epoch, train_loss, val_loss, val_R2)")
    pprint(results[lr]["history"])

w_scaled = best_w          # (5,) tensor
b_scaled = best_b          # scalar float

# Undo scaling
W_real = (w_scaled.numpy() / x_scaler.scale_) * y_scaler.scale_
B_real = (
    y_scaler.mean_
    + (b_scaled * y_scaler.scale_)
    - (x_scaler.mean_ / x_scaler.scale_) @ W_real
)

print("\nReal Model Parameters")
for feature, w in zip(features, W_real):
    print(f"{feature}: {w:.2f}")

print(f"Bias (B): {float(B_real):.2f}")
